{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Direct Reinforcement Learning for Financial Signal Representation and Trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "資産取引において練達したトレーダーを打ち負かすことができるほどにコンピューターを訓練することはできるのか。この論文では再帰型深層ニューラルネットワークを"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "このモデルにおける取引＝市場の状況把握と最適行動の二つ逐次的意思決定からなる。  \n",
    "従来手法と比べてこの動的な意思決定モデルは熟練したトレーダーの情報がない分だけ挑戦的。  \n",
    "・・・状況を探索して各期の最適行動を決定する必要がある。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "自己学習と強化学習(RL)。\n",
    "確率的最適制御と強化学習の研究。いくつかの事例では強化学習を備えたコンピューターが人間の能力を凌駕することもある。 \n",
    "・・・トレーディングにおいてもRLが人間に勝つことができるのではないか？  \n",
    "but トレーディングにおけるRLには従来のRLが対象にしてきた課題にはない問題がある。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "①金融環境の把握と表現の困難さから生じる問題。  \n",
    "金融データには膨大な数のノイズ、ジャンプ、そして時系列を非定常なものに導くような運動が含まれている。  \n",
    "移動平均やstochastic technical indicatorsが市場状態の評価のために用いられている。  \n",
    "but drawback of technical analysis is its poor generalization ability.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "②トレーディングのダイナミックな意思決定から生じる問題。  \n",
    "注文の発注は様々な要因を考慮に入れた上でのシステマテック的な仕事である。  \n",
    "頻繁な取引ポジションの変更は利益を生まないだけでなくTransaction costやslippageから多大なロスを生む。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "この二つの問題に取り組むためにRDNN構造を、①の環境への反応問題と、②の意思決定問題に対処するために導入する。  \n",
    "RDNN $\\approx$ DNN for feature learning + RNN for market summarization  \n",
    "※ To further improve the robustness for market summarization, the fuzzy learining concepts are introduced to reduce the uncertainty of the input data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Direct Deep Reinforcement Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Direct Reinforcement Trading \n",
    "= 離散選択モデルで政策を決め動的計画法で解く。  \n",
    "typical DRL is essentially a one layer RNN.  \n",
    "$p_1, p_2, ..., p_t, ...$ : price sequences released from the exchange center.  \n",
    "return : $ z_t = p_t - p_{t-1} $  \n",
    "real-time trading decision : $ \\delta_t \\in \\{long, neutral, short\\} = \\{1, 0, -1\\} $  \n",
    "profit : $ R_t = \\delta_{t-1} z_t - c|\\delta_t - \\delta_{t-1}| $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "profit = value function in each time point  \n",
    "the accumulated value throughout the whole training period can ve defined as  \n",
    "$$ max_\\Theta U_T \\{R_1...R_T|\\Theta\\} $$\n",
    "簡単化のため、ここでは全体のvalue functionを各時点でのvalue functionの和として話を進める。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the primary problem : how to solve it efficiently.  \n",
    "従来の強化学習では離散空間に価値関数を設定してDPで繰り返し計算で解いていた。  \n",
    "しかし、動的な取引の問題を、限られた数の離散空間で価値関数を学習させることで直接解くことは難しい。  \n",
    "よって価値関数ではなく（本来価値関数があたえられた上でそれを最大化するように選ばれることで決まる）政策関数を学習する戦略を採用する。  \n",
    "=== DRL  \n",
    "a nonlinear function is adopted in DRL to approximate the trading action at each time point by\n",
    "$$ \\delta_t = tanh[<w, f_t> + b + u\\delta_{t-1}] $$\n",
    "f_t : feature vector of the current market condition at time t  \n",
    "(w, b) are the coefficients for the feature regression  \n",
    "つまり、回帰式にpenaltyを入れてtanhで表現している。bはbias項だがつまり1への回帰である。    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Fig1a.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In DRL, the recent m return values are directly adopted as the feature vector  \n",
    "$$ f_t = [z_{t−m+1}, . . . , z_t] ∈ R^m. $$\n",
    "In addition to the features, another term \n",
    "$$ uδ_{t−1} $$\n",
    "is also added into the regression to take the latest trading decision into consideration.   \n",
    "This term is used to discourage the agent to frequently change the trading positions and, hence, to avoid heavy TCs.  \n",
    "With the linear transformation in the brackets, tanh(·) further maps the function into the range of (−1, 1) to approximate the final trading decision.   \n",
    "The optimization of DRL aims to learn such a family of parameter set\n",
    "$$ \\Theta = \\{w, u, b\\} $$\n",
    "that can maximize the global reward function in  \n",
    "$$ max_\\Theta U_T \\{R_1...R_T|\\Theta\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B Deep Recurrent Neural Network for DDR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "biasはfeatureに1を加えて回帰することで省略。  \n",
    "DRLNNは自分の選択$\\delta_t$を再帰的に利用。  \n",
    "RNNを使うことは長期記憶を導入することであり過去の選択を継承できる。  \n",
    "ただし、結局のところfeatureから市況の要約がうまくできない。回帰一般の問題。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement feature learning, in this paper, we introduce the prevalent DL into DRL for simultaneously feature learning and dynamic trading.   \n",
    "DL is a very powerful feature learning framework whose potentials have been extensively demonstrated in a number of machine learning problems.  \n",
    "In detail, DL constructs a DNN to hierarchically transform the information from layer to layer.   \n",
    "Such deep representation encourages much informative feature representations for a specific learning task.   \n",
    "階層表現はより豊かに市況を表現できる（つまり非線形かつノンパラな手法）  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Fig1b.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の青部分がDLの階層表現。  \n",
    "By extending DL into DRL, the feature learning part(blue panel) is added to the RNN.  \n",
    "DLを非線形な関数による近似と考えると、\n",
    "$$ F_t = g_d(f_t) $$\n",
    "であり、　　\n",
    "$$ \\delta_t = tanh[<w, F_t> + b + u\\delta_{t-1}] $$\n",
    "l+1層のそれぞれのノードはl層のすべてのノードと繋がっている。  \n",
    "$a^l_i$をl層におけるi番目の入力とし、$o^l_i$を対応する出力とする。  \n",
    "$$ a^l_i = <w^l_i, o^{(l-1)}> + b^l_i $$\n",
    "$$ o^l_i = \\frac{1}{1+e^{-a^l_i}} $$\n",
    "標準シグモイド関数。  \n",
    "$ o^{l-1}_i $はl-1層からの出力のベクトル。すべてのノードからの出力に重みが乗って入力される。  \n",
    "4層の隠れ層に、それぞれ128ノード。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C Fuzzy Extensions to Reduce Uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deep configuration well addresses the feature learning task in the RNN.   \n",
    "↔︎ However, another important issue,   \n",
    "#### i.e., data uncertainty in financial data, should also be carefully considered.  \n",
    "Financial sequences contain high amount of unpredictable uncertainty due to the random gambling behind trading.  \n",
    "Besides, a number of other factors, e.g., global economic atmosphere and some company rumors, may also affect the direction of the financial signal in real time.   \n",
    "Therefore, reducing the uncertainties in the raw data is an important approach to increase the robustness for financial signal mining.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the artificial intelligence community, fuzzy learning is an ideal paradigm to reduce the uncertainty in the original data.  \n",
    "Rather than adopting precise descriptions of some phenomena, fuzzy systems prefer to assign fuzzy linguist values to the input data.     \n",
    "#### Such fuzzified representations can be easily obtained by comparing the real-world data with a number of fuzzy rough sets and then deriving the corresponding fuzzy membership degrees.   \n",
    "#### Consequently, the learning system only works with these fuzzy representations to make robust control decisions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ラフ集合(wikipedia)\n",
    "ラフ集合（ラフしゅうごう、Rough sets）とは上近似集合と下近似集合からなる集合で、非数値の対象を粗く（ラフに）記述することができるものである。これを用いることによって、他のデータマイニング手法からは得られにくい、非数値であったり矛盾のあるようなデータからの知識獲得が可能である。Rough sets theory（ラフ集合理論）の頭文字をとって RST や、Rough sets approach（ラフ集合アプローチ）の頭文字をとって RSA とも呼ばれる。\n",
    "応用として、対象集合をファジィ集合に拡張したファジィ-ラフ集合理論 (Fuzzy-Rough sets theory) というものがある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なるほど。ではfuzzy rough setsとは何か。  \n",
    "For the financial problem discussed here, the fuzzy rough sets can be naturally defined according to the basicmovements of the stock price.   \n",
    "In detail, the fuzzy sets are defined on the increasing, decreasing, and the no trend groups.  \n",
    "The parameters in the fuzzy membership function can then be predefined according to the context of the discussed problem.  \n",
    "Alternatively, they could be learned in a fully data-driven manner.   \n",
    "The financial problem is highly complicated and it is hard to manually set up the fuzzy membership functions\n",
    "Fig. 2.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Fig2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Overview of fuzzy DRNNs for robust feature learning and self-taught\n",
    "trading.\n",
    "according to the experiences. Therefore, we prefer to directly\n",
    "learn the membership functions and this idea will be detailed\n",
    "in Section IV.\n",
    "In fuzzy neural networks, the fuzzy representation part is\n",
    "conventionally connected to the input vector ft (green nodes)\n",
    "with different membership functions [35]. To note, in our\n",
    "setting, we follow a pioneering work [35] to assign k different\n",
    "fuzzy degrees to each dimension of the input vector. In the\n",
    "cartoon of Fig. 2, only two fuzzy nodes (k = 2) are connected\n",
    "to each input variable due to the space limitation. In our practical\n",
    "implementation, k is fixed as 3 to describe the increasing,\n",
    "decreasing, and no trend conditions. Mathematically, the\n",
    "i th fuzzy membership function vi (·) : R → [0, 1] maps the\n",
    "i th input as a fuzzy degree\n",
    "o(l)\n",
    "i = vi\n",
    "$\n",
    "a(l)\n",
    "i\n",
    "%\n",
    "= e−\n",
    "$\n",
    "a(l)\n",
    "i −mi\n",
    "%2\n",
    "/σ2\n",
    "i &i. (7)\n",
    "The Gaussian membership function with mean m and variance\n",
    "σ2 is utilized in our system following the suggestions\n",
    "of [37] and [38]. After getting the fuzzy representations, they\n",
    "are directly connected to the deep transformation layer to seek\n",
    "for the deep transformations.\n",
    "In conclusion, the fuzzy DRNN (FDRNN) is composed of\n",
    "three major parts as fuzzy representation, deep transformation,\n",
    "and DRT.When viewing the FDRNN as a unified system, these\n",
    "three parts, respectively, play the roles of data preprocessing\n",
    "(reduce uncertainty), feature learning (deep transformation),\n",
    "and trading policy making (RL). The whole optimization\n",
    "framework is given as follows:\n",
    "max\n",
    "{\",gd (·),v(·)}\n",
    "UT (R1..RT )\n",
    "s.t. Rt = δt−1zt − c|δt − δt−1|\n",
    "δt = tanh(⟨w, Ft⟩+b + uδt−1)\n",
    "Ft = gd(v(ft )) (8)\n",
    "where there are three groups of parameters to be learned,\n",
    "i.e., the trading parameters \" = (w, b, u), fuzzy representations\n",
    "v(·), and deep transformations gd (·). In the above\n",
    "optimization, UT is the ultimate reward of the RL function,\n",
    "δt is the policy approximated by the FRDNN, and Ft is\n",
    "the high-level feature representat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4 DRNN LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ max_{\\{\\Theta, g_d(·), v(·)\\}} U_T(R_1, R_T) $$\n",
    "$$ s.t. R_t = \\delta_{t-1}z_t - c|\\delta_t - \\delta_{t-1}) $$\n",
    "$$ \\delta_t = tanh(<w, F_t> + b + u\\delta_{t-1}) $$\n",
    "$$ F_t = g_d(v(f_t)) $$\n",
    "\n",
    "はコンセプト的にはエレガントである。  \n",
    "しかし、不幸にも相対的に最適化は困難でる。  \n",
    "これは構成されたDNNが幾千もの隠れパラメーターを持ち、それらをinferしなければならないためである。  \n",
    "このセクションでは、私たちは実践的な学習戦略を提示し、Aシステム初期化とBチューニングの2stepsでDNNをtrainする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A System Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B Task-Aware BPTT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Fig3.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
