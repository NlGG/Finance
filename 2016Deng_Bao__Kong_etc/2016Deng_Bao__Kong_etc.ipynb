{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Direct Reinforcement Learning for Financial Signal Representation and Trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "資産取引において練達したトレーダーを打ち負かすことができるほどにコンピューターを訓練することはできるのか。この論文では再帰型深層ニューラルネットワークを"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "このモデルにおける取引＝市場の状況把握と最適行動の二つ逐次的意思決定からなる。  \n",
    "従来手法と比べてこの動的な意思決定モデルは熟練したトレーダーの情報がない分だけ挑戦的。  \n",
    "・・・状況を探索して各期の最適行動を決定する必要がある。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "自己学習と強化学習(RL)。\n",
    "確率的最適制御と強化学習の研究。いくつかの事例では強化学習を備えたコンピューターが人間の能力を凌駕することもある。 \n",
    "・・・トレーディングにおいてもRLが人間に勝つことができるのではないか？  \n",
    "but トレーディングにおけるRLには従来のRLが対象にしてきた課題にはない問題がある。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "①金融環境の把握と表現の困難さから生じる問題。  \n",
    "金融データには膨大な数のノイズ、ジャンプ、そして時系列を非定常なものに導くような運動が含まれている。  \n",
    "移動平均やstochastic technical indicatorsが市場状態の評価のために用いられている。  \n",
    "but drawback of technical analysis is its poor generalization ability.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "②トレーディングのダイナミックな意思決定から生じる問題。  \n",
    "注文の発注は様々な要因を考慮に入れた上でのシステマテック的な仕事である。  \n",
    "頻繁な取引ポジションの変更は利益を生まないだけでなくTransaction costやslippageから多大なロスを生む。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "この二つの問題に取り組むためにRDNN構造を、①の環境への反応問題と、②の意思決定問題に対処するために導入する。  \n",
    "RDNN $\\approx$ DNN for feature learning + RNN for market summarization  \n",
    "※ To further improve the robustness for market summarization, the fuzzy learining concepts are introduced to reduce the uncertainty of the input data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Direct Deep Reinforcement Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Direct Reinforcement Trading \n",
    "= 離散選択モデルで政策を決め動的計画法で解く。  \n",
    "typical DRL is essentially a one layer RNN.  \n",
    "$p_1, p_2, ..., p_t, ...$ : price sequences released from the exchange center.  \n",
    "return : $ z_t = p_t - p_{t-1} $  \n",
    "real-time trading decision : $ \\delta_t \\in \\{long, neutral, short\\} = \\{1, 0, -1\\} $  \n",
    "profit : $ R_t = \\delta_{t-1} z_t - c|\\delta_t - \\delta_{t-1}| $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "profit = value function in each time point  \n",
    "the accumulated value throughout the whole training period can ve defined as  \n",
    "$$ max_\\Theta U_T \\{R_1...R_T|\\Theta\\} $$\n",
    "簡単化のため、ここでは全体のvalue functionを各時点でのvalue functionの和として話を進める。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the primary problem : how to solve it efficiently.  \n",
    "従来の強化学習では離散空間に価値関数を設定してDPで繰り返し計算で解いていた。  \n",
    "しかし、動的な取引の問題を、限られた数の離散空間で価値関数を学習させることで直接解くことは難しい。  \n",
    "よって価値関数ではなく（本来価値関数があたえられた上でそれを最大化するように選ばれることで決まる）政策関数を学習する戦略を採用する。  \n",
    "=== DRL  \n",
    "a nonlinear function is adopted in DRL to approximate the trading action at each time point by\n",
    "$$ \\delta_t = tanh[<w, f_t> + b + u\\delta_{t-1}] $$\n",
    "f_t : feature vector of the current market condition at time t  \n",
    "(w, b) are the coefficients for the feature regression  \n",
    "つまり、回帰式にpenaltyを入れてtanhで表現している。bはbias項だがつまり1への回帰である。    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Fig1a.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In DRL, the recent m return values are directly adopted as the feature vector  \n",
    "$$ f_t = [z_{t−m+1}, . . . , z_t] ∈ R^m. $$\n",
    "In addition to the features, another term \n",
    "$$ uδ_{t−1} $$\n",
    "is also added into the regression to take the latest trading decision into consideration.   \n",
    "This term is used to discourage the agent to frequently change the trading positions and, hence, to avoid heavy TCs.  \n",
    "With the linear transformation in the brackets, tanh(·) further maps the function into the range of (−1, 1) to approximate the final trading decision.   \n",
    "The optimization of DRL aims to learn such a family of parameter set\n",
    "$$ \\Theta = \\{w, u, b\\} $$\n",
    "that can maximize the global reward function in  \n",
    "$$ max_\\Theta U_T \\{R_1...R_T|\\Theta\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B Deep Recurrent Neural Network for DDR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "biasはfeatureに1を加えて回帰することで省略。  \n",
    "DRLNNは自分の選択$\\delta_t$を再帰的に利用。  \n",
    "RNNを使うことは長期記憶を導入することであり過去の選択を継承できる。  \n",
    "ただし、結局のところfeatureから市況の要約がうまくできない。回帰一般の問題。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement feature learning, in this paper, we introduce the prevalent DL into DRL for simultaneously feature learning and dynamic trading.   \n",
    "DL is a very powerful feature learning framework whose potentials have been extensively demonstrated in a number of machine learning problems.  \n",
    "In detail, DL constructs a DNN to hierarchically transform the information from layer to layer.   \n",
    "Such deep representation encourages much informative feature representations for a specific learning task.   \n",
    "階層表現はより豊かに市況を表現できる（つまり非線形かつノンパラな手法）  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Fig1b.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の青部分がDLの階層表現。  \n",
    "By extending DL into DRL, the feature learning part(blue panel) is added to the RNN.  \n",
    "DLを非線形な関数による近似と考えると、\n",
    "$$ F_t = g_d(f_t) $$\n",
    "であり、　　\n",
    "$$ \\delta_t = tanh[<w, F_t> + b + u\\delta_{t-1}] $$\n",
    "l+1層のそれぞれのノードはl層のすべてのノードと繋がっている。  \n",
    "$a^l_i$をl層におけるi番目の入力とし、$o^l_i$を対応する出力とする。  \n",
    "$$ a^l_i = <w^l_i, o^{(l-1)}> + b^l_i $$\n",
    "$$ o^l_i = \\frac{1}{1+e^{-a^l_i}} $$\n",
    "標準シグモイド関数。  \n",
    "$ o^{l-1}_i $はl-1層からの出力のベクトル。すべてのノードからの出力に重みが乗って入力される。  \n",
    "4層の隠れ層に、それぞれ128ノード。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C Fuzzy Extensions to Reduce Uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deep configuration well addresses the feature learning task in the RNN.   \n",
    "↔︎ However, another important issue,   \n",
    "#### i.e., data uncertainty in financial data, should also be carefully considered.  \n",
    "Financial sequences contain high amount of unpredictable uncertainty due to the random gambling behind trading.  \n",
    "Besides, a number of other factors, e.g., global economic atmosphere and some company rumors, may also affect the direction of the financial signal in real time.   \n",
    "Therefore, reducing the uncertainties in the raw data is an important approach to increase the robustness for financial signal mining.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the artificial intelligence community, fuzzy learning is an ideal paradigm to reduce the uncertainty in the original data.  \n",
    "Rather than adopting precise descriptions of some phenomena, fuzzy systems prefer to assign fuzzy linguist values to the input data.     \n",
    "#### Such fuzzified representations can be easily obtained by comparing the real-world data with a number of fuzzy rough sets and then deriving the corresponding fuzzy membership degrees.   \n",
    "#### Consequently, the learning system only works with these fuzzy representations to make robust control decisions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ラフ集合(wikipedia)\n",
    "ラフ集合（ラフしゅうごう、Rough sets）とは上近似集合と下近似集合からなる集合で、非数値の対象を粗く（ラフに）記述することができるものである。これを用いることによって、他のデータマイニング手法からは得られにくい、非数値であったり矛盾のあるようなデータからの知識獲得が可能である。Rough sets theory（ラフ集合理論）の頭文字をとって RST や、Rough sets approach（ラフ集合アプローチ）の頭文字をとって RSA とも呼ばれる。\n",
    "応用として、対象集合をファジィ集合に拡張したファジィ-ラフ集合理論 (Fuzzy-Rough sets theory) というものがある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "なるほど。ではfuzzy rough setsとは何か。  \n",
    "For the financial problem discussed here, the fuzzy rough sets can be naturally defined according to the basicmovements of the stock price.   \n",
    "In detail, the fuzzy sets are defined on the increasing, decreasing, and the no trend groups.  \n",
    "The parameters in the fuzzy membership function can then be predefined according to the context of the discussed problem.  \n",
    "Alternatively, they could be learned in a fully data-driven manner.   \n",
    "The financial problem is highly complicated and it is hard to manually set up the fuzzy membership functions according to the experiences.   \n",
    "#### Therefore, we prefer to directly learn the membership functions and this idea will be detailed in Section IV.  \n",
    "直接membership functionsも推定したいよね。  \n",
    "In fuzzy neural networks, the fuzzy representation part is conventionally connected to the input vector ft (green nodes) with different membership functions [35].   \n",
    "To note, in our setting, we follow a pioneering work [35] to assign k different fuzzy degrees to each dimension of the input vector.   \n",
    "特徴ベクトルにk個のfuzzyを接続。  \n",
    "In the cartoon of Fig. 2, only two fuzzy nodes (k = 2) are connected\n",
    "to each input variable due to the space limitation.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Fig2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In our practical implementation, k is fixed as 3 to describe the increasing, decreasing, and no trend conditions.   Mathematically, the i th fuzzy membership function \n",
    "$$ v_i (·) : R → [0, 1] $$\n",
    "maps the i th input as a fuzzy degree\n",
    "$$ o^{(l)}_i = v_i(a^{(l)}_i) = e^{(−a^{(l)}_i−m_i)^2/σ^2_i} \\forall i. (7) $$\n",
    "The Gaussian membership function with mean m and variance\n",
    "σ2 is utilized in our system following the suggestions\n",
    "of [37] and [38]. After getting the fuzzy representations, they\n",
    "are directly connected to the deep transformation layer to seek\n",
    "for the deep transformations.\n",
    "In conclusion, the fuzzy DRNN (FDRNN) is composed of\n",
    "three major parts as fuzzy representation, deep transformation,\n",
    "and DRT.When viewing the FDRNN as a unified system, these\n",
    "three parts, respectively, play the roles of data preprocessing\n",
    "(reduce uncertainty), feature learning (deep transformation),\n",
    "and trading policy making (RL). The whole optimization\n",
    "framework is given as follows:\n",
    "max\n",
    "{\",gd (·),v(·)}\n",
    "UT (R1..RT )\n",
    "s.t. Rt = δt−1zt − c|δt − δt−1|\n",
    "δt = tanh(⟨w, Ft⟩+b + uδt−1)\n",
    "Ft = gd(v(ft )) (8)\n",
    "where there are three groups of parameters to be learned,\n",
    "i.e., the trading parameters \" = (w, b, u), fuzzy representations\n",
    "v(·), and deep transformations gd (·). In the above\n",
    "optimization, UT is the ultimate reward of the RL function,\n",
    "δt is the policy approximated by the FRDNN, and Ft is\n",
    "the high-level feature representat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4 DRNN LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ max_{\\{\\Theta, g_d(·), v(·)\\}} U_T(R_1, R_T) $$\n",
    "$$ s.t. R_t = \\delta_{t-1}z_t - c|\\delta_t - \\delta_{t-1}) $$\n",
    "$$ \\delta_t = tanh(<w, F_t> + b + u\\delta_{t-1}) $$\n",
    "$$ F_t = g_d(v(f_t)) $$\n",
    "\n",
    "はコンセプト的にはエレガントである。  \n",
    "しかし、不幸にも相対的に最適化は困難でる。  \n",
    "これは構成されたDNNが幾千もの隠れパラメーターを持ち、それらをinferしなければならないためである。  \n",
    "このセクションでは、私たちは実践的な学習戦略を提示し、Aシステム初期化とBチューニングの2stepsでDNNをtrainする。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A System Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習の三段階に対してそれぞれパラメーター初期化の戦略を提示する。  \n",
    "\n",
    "#### The fuzzy representation part [Fig. 2 (purple panel)] \n",
    "The only parameters to be specified are the fuzzy centers ($m_i$) and widths ($σ^2_i$) of the fuzzy nodes, where i means the i th node of the fuzzy membership layer.   \n",
    "We directly apply k-means to divide the training samples into k classes.   \n",
    "The parameter k is fixed as 3, because each input node is connected with three membership functions.   \n",
    "Then, in each cluster, the mean and variance of each dimension on the input vector ($f_t$) are sequentially calculated to initialize the corresponding $m_i$ and $σ^2_i$ .\n",
    "\n",
    "#### The deep transformation part [Fig. 2 (blue panel)]\n",
    "The AE is adopted to initialize the deep transformation part\n",
    "in Fig. 2 (blue panel). In a nutshell, AE aims at optimally\n",
    "reconstructing the input information on a virtual layer placed\n",
    "after the hidden representations. For ease of explanation, three\n",
    "layers are specified here, i.e., the (l)th input layer, the (l+1)th\n",
    "hidden layer, and the (l+2)th reconstruction layer. These three\n",
    "layers are all well connected. We define hθ (·) [respectively,\n",
    "hγ (·)] as the feedforward transformation from the lth to\n",
    "(l + 1)th layer [respectively, (l + 1)th to (l + 2)th layer]\n",
    "with parameter set θ (respectively, γ ). The AE optimization\n",
    "minimizes the following loss: &\n",
    "t\n",
    "''\n",
    "x(l)\n",
    "t − hγ\n",
    "$\n",
    "hθ\n",
    "$\n",
    "x(l)\n",
    "t\n",
    "%%''\n",
    "2\n",
    "2 + η∥w(l+1)∥22\n",
    ". (9)\n",
    "To note, x(l)\n",
    "t are the nodes’ statuses of the lth layer with\n",
    "the tth training sample as input. In (9), a quadratic term\n",
    "is added to avoid the overfitting phenomena. After solving\n",
    "the AE optimization, parameter set θ = {w(l+1), b(l+1)} is\n",
    "recorded in the network as the initialized parameter of the\n",
    "(l +1)th layer. The reconstruction layer and its corresponding\n",
    "parameters γ are not used. This is because the reconstruction\n",
    "layer is just a virtual layer, assisting parameter learning of the\n",
    "hidden layer [28], [39]. The AE optimizations are implemented\n",
    "on each hidden layer sequentially until all the parameters in\n",
    "the deep transformation part have been set up.\n",
    "\n",
    "#### The DRL part, \n",
    "\n",
    "the parameters can be initialized using final\n",
    "deep representation Ft as the input to the DRL model. This\n",
    "process is equivalent to solving the shallow RNN in Fig. 1(a),\n",
    "which has been discussed in [17]. It is noted that all the learning\n",
    "strategies presented in this section are all about parameter\n",
    "initializations. In order to make the whole DL system perform\n",
    "robustly in addressing difficult tasks, a fine tuning step is\n",
    "required to precisely adjust the parameters of each layer. This\n",
    "fine tuning step can be considered as task-dependent feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B Task-Aware BPTT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the conventional way, the error BP method is applied\n",
    "to the DNN fine tuning step. However, the FRDNN is bit\n",
    "complicated that exhibits both recurrent and deep structures.\n",
    "We denote θ as the general parameter in the FRDNN, and its\n",
    "gradient is easily calculated by the chain rule\n",
    "∂UT\n",
    "∂θ =\n",
    "&\n",
    "t\n",
    "dUt\n",
    "dRt\n",
    "(\n",
    "dRt\n",
    "dδt\n",
    "dδt\n",
    "dθ +\n",
    "dRt\n",
    "dδt−1\n",
    "dδt−1\n",
    "dθ\n",
    ")\n",
    "dδt\n",
    "dθ =\n",
    "∂δt\n",
    "∂θ +\n",
    "∂δt\n",
    "∂δt−1\n",
    "dδt−1\n",
    "dθ\n",
    ". (10)\n",
    "From (10), it is apparent when deriving the gradient dδt/dθ,\n",
    "one should recursively calculate the gradient for dδt−τ /dθ,\n",
    "&τ = 1, . . . , T .1 Such recursive calculation inevitably imposes\n",
    "great difficulties for gradient derivations. To simplify the\n",
    "problem, we introduce the famous BPTT [40] method to cope\n",
    "with the recurrent structure of the NN.\n",
    "By analyzing the FRDNN structure in Fig. 2, the recurrent\n",
    "link comes from the output side to the input side, i.e., δt−1 is\n",
    "used as the input of the neuron to calculate δt. Fig. 3 shows\n",
    "the first two-step unfolding of the FRDNN. We call each block\n",
    "with different values of τ as a time stack, and Fig. 3 shows two\n",
    "time stacks (with τ = 0 and τ = 1). After the BPTT unfolds,\n",
    "the current system does not involve any recurrent structure and\n",
    "the typical BP method is easily applied to it. When getting\n",
    "parameters’ gradients at each separate time stack, they are\n",
    "averaged together forming the final gradient of each parameter.\n",
    "According to Fig. 3, the original DNN becomes even deeper\n",
    "due to the implementations of time-based unfolding. To clarify\n",
    "this point, we remind the readers to notice the time stacks\n",
    "after expansion. It leads to a deep structure along different\n",
    "time delays. Moreover, every time stack (with different values\n",
    "of τ) contains its own deep feature learning part.When directly\n",
    "applying the BPTT, the gradient vanish on deep layers is not\n",
    "avoided in the fine-tuning step [41]. This problem becomes\n",
    "even worse on the high-order time stacks and the front layers.\n",
    "To solve the aforementioned problem, we propose a more\n",
    "practical solution to bring the gradient information directly\n",
    "from the learning task to each time stack and each layer of\n",
    "the DL part. In the time unfolding part, the red dotted lines\n",
    "1In"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Fig3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "are connected from the task UT to the output node of each\n",
    "time stack. With this setting, the back-propagated gradient\n",
    "information of each tim\n",
    "e stack comes from two respective\n",
    "parts: 1) the previous time stack (lower order time delay) and\n",
    "2) the reward function (learning task). Similarly, the gradient\n",
    "of the output node in each time stack is brought back to the\n",
    "DL layers by the green dotted line. Such a BPTT method with\n",
    "virtual lines connecting with the objective function is termed\n",
    "task-aware BPTT.\n",
    "The detailed process to train the FRDNN has been summarized\n",
    "in Algorithm 1. In the algorithm, we denote \" as the\n",
    "general symbol to represent parameters. It represents the whole\n",
    "latent parameters’ family involved in the FRDNN. Before the\n",
    "gradient decreasing implementation in line 10, the calculated\n",
    "gradient vector is further normalized to avoid extremely large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "V. EXPERIMENTAL VERIFICATIONS\n",
    "A. Experimental Setup\n",
    "We test the DDR trading model on the real-world financial\n",
    "data. Both the stock index and commodity future contracts\n",
    "are tested in this section. For the stock-index data, we select\n",
    "the stock-IF contract, which is the first index-based future\n",
    "contract traded in China. The IF data is calculated based on\n",
    "the prices of the top 300 stocks from both Shanghai and\n",
    "Shenzhen exchange centers. The IF future is the most liquid\n",
    "one and occupies the heaviest trading volumes among all\n",
    "the future contracts in China. On the commodity market, the\n",
    "silver (AG) and sugar (SU) contracts are used, because both\n",
    "of them exhibit very high liquidity, allowing trading actions\n",
    "to be executed in almost real time. All these contracts allow\n",
    "Fig. 4. Prices (based on minute resolutions) of the three tested future\n",
    "contracts. Red parts: RDNN initializations. Blue parts: out-of-sample tests.\n",
    "TABLE I\n",
    "SUMMARY OF SOME PRACTICAL PROPERTIES\n",
    "OF THE TRADED CONTRACTS\n",
    "both short and long operations. The long (respectively, short)\n",
    "position makes profits when the subsequent market price goes\n",
    "higher (respectively, lower).\n",
    "The financial data are captured by our own trading system\n",
    "in each trading day and the historic data is maintained in\n",
    "a database. In our experiment, the minute-level close prices\n",
    "are used, implying that there is a 1-min interval between the\n",
    "price pt and pt+1. The historic data of the three contracts in\n",
    "the minute resolutions are shown in Fig. 4. In this one-year\n",
    "period, the IF contracts accumulate more ticks than commodity\n",
    "data because the daily trading period of IF is much longer than\n",
    "commodity contracts.\n",
    "From Fig. 4, it is also interesting to note that these three contracts\n",
    "exhibit quite different market patterns. IF data get very\n",
    "large upward and downward movements in the tested period.\n",
    "The AG contract, generally, shows a downward trend and the\n",
    "SU has no obvious direction in the testing period. For practical\n",
    "usage, some other issues related to trading should also be\n",
    "considered. We have summarized some detailed information\n",
    "about these contracts in Table I. The inherent values of these\n",
    "three contracts are evaluated by China Yuan (CNY) per point\n",
    "(CNY/pnt). For instance, in the IF data, the increase (decrease)\n",
    "in one point may lead to a reward of 300 CNY for a long\n",
    "(respectively, short) position and vice versa. The TCs charged\n",
    "by the brokerage company is also provided. By considering\n",
    "other risky factors, a much higher c is set in (1). It is five\n",
    "times higher than the real TCs.\n",
    "The raw price changes of the last 45 min and the momentum\n",
    "change to the previous 3 h, 5 h, 1 day, 3 days, and 10 days\n",
    "is directly used as the input of the trading system (ft ∈ R50).\n",
    "In the fuzzy learning part, each of the 50 input nodes are\n",
    "connected with three fuzzy membership functions to seek for\n",
    "the first-level fuzzy representation in R150. Then, the fuzzy\n",
    "layer is sequentially passed through four deep transformation\n",
    "layers with 128, 128, 128, and 20 hidden nodes per layer. The\n",
    "feature representation (Ft ∈ R20) of the final deep layer is\n",
    "connected with the DRL part for trading policy making.\n",
    "B. Details on Deep Training\n",
    "In this section, we discuss some details related to deep\n",
    "training. In practice, the system is trained by two sequential\n",
    "This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\n",
    "DENG et al.: DDR LEARNING FOR FINANCIAL SIGNAL REPRESENTATION AND TRADING 7\n",
    "steps of initialization and online updating. In the initialization\n",
    "step, the first 15 000 time points of each time series\n",
    "in Fig. 4 (red parts) are employed for system warming up.\n",
    "It is noted that these initialization data will not be used for\n",
    "out-of-sample tests. After initialization, the parameters in the\n",
    "RDNN are iteratively updated in an online manner with the\n",
    "recently released data. The online updating strategy allows\n",
    "the model to get aware of the latest market condition and\n",
    "revise its parameters accordingly.\n",
    "In practice, the first 15 000 time points are used to set up the\n",
    "RDNN and the well-trained system is exploited to trade the\n",
    "time points from 15 001 to 20 000. Then, the sliding window of\n",
    "the training data is moved 5000 ticks forward covering a new\n",
    "training set from 5000 to 20 000. As indicated in Section IV,\n",
    "the training phase of the RDNN is composed of two main\n",
    "steps of layerwise parameter initialization and fine tuning. It is\n",
    "clarified here that the parameter initialization implementations\n",
    "are only performed in the first round of training, i.e., on the\n",
    "first 15 000 ticks. With the sliding window of the training\n",
    "set moving ahead, the optimal parameters obtained from\n",
    "the last training round are directly used as the initialized\n",
    "values.\n",
    "FDDR is a highly nonconvex system and only a local minimum\n",
    "is expected after convergence. Besides, the overfitting\n",
    "phenomenon is a known drawback faced by most DNNs.\n",
    "To mitigate the disturbances of overfitting, we adopt two\n",
    "convenient strategies that have been proved to be powerful in\n",
    "practice. The first strategy is the widely used early stopping\n",
    "method for DNN training. The system is only trained for\n",
    "100 epochs with a gradient decreasing parameter be ηc = 0.97\n",
    "in Algorithm 1. Second, we conduct model selection to select\n",
    "a good model for the out-of-sample data. To achieve this\n",
    "goal, 15 000 training points are divided into two sets as\n",
    "RDNN training set (first 12 000) and validation set (last 3000).\n",
    "On the first 12 000 time points, FDDR is trained for 5 times\n",
    "and the best one is selected on the next 3000 blind points.\n",
    "Such validation helps to exclude some highly overfitted NNs\n",
    "from the training set.\n",
    "Another challenge in training RDNN comes from the\n",
    "gradient vanishing issue, and we have introduced the\n",
    "task-aware BPTT method to cope with this problem. To prove\n",
    "its effectiveness, we show the training performance with\n",
    "the comparison with the typical BPTT method. The trading\n",
    "model is trained on the first 12 000 points of the IF data\n",
    "in Fig. 4(a). The objective function values (accumulated\n",
    "rewards) of the two methods along with the training epochs\n",
    "are shown in Fig. 5. From the comparisons, it is apparent\n",
    "that the task-aware BPTT outperforms the BPTT method in\n",
    "making more rewards (accumulated trading profits). Moreover,\n",
    "the task-aware BPTT requires less iterative steps for\n",
    "convergence.\n",
    "C. General Evaluations\n",
    "In this section, we evaluate the DDR trading system on\n",
    "practical data. The system is compared with other RL systems\n",
    "for online trading. The first competitor is the DRL system [17].\n",
    "Besides, the sparse coding-inspired optimal training (SCOT)\n",
    "Fig. 5. Training epochs and the corresponding rewards by training the DRNN\n",
    "by (a) task-aware BPTT and (b) normal BPTT. The training data are the first\n",
    "12 000 data points in Fig. 4(a).\n",
    "system [19] is also evaluated, which uses shallow feature\n",
    "learning part of sparse coding.2 Finally, the results of DDR\n",
    "and its FDDR are also reported.\n",
    "In the previous discussions, the reward function defined\n",
    "for RL is regarded as the TPs gained in the training period.\n",
    "Compared with total TP, in modern portfolio theory, the riskadjusted\n",
    "profits are more widely used to evaluate a trading\n",
    "system’s performance. In this paper, we will also consider an\n",
    "alternative reward function into the RL part, i.e., SR, which\n",
    "has been widely used in many trading related works [42], [43].\n",
    "The SR is defined as the ratio of average return to standard\n",
    "deviation of the returns calculated in period 1, . . . , T ,\n",
    "i.e., USR\n",
    "T = (mean(Rt )/std(Rt )). To simplify the expression,\n",
    "we follow the same idea in DRL [17] to use the moving\n",
    "SR instead. In general, moving SR gets the first-order Taylor\n",
    "expansion of typical SR and then updates the value in an\n",
    "incremental manner. Please refer to [17, Sec. 2.4] for detailed\n",
    "derivations. Different trading systems are trained with both\n",
    "TP and SR as the RL objectives. The details of the profit\n",
    "and loss (P&L) curves are shown in Fig. 6. The quantitative\n",
    "evaluations are summarized in Table II, where the testing\n",
    "performances are also reported in the forms of TP and SR. The\n",
    "performance of buying and holding (B&H) is also reported in\n",
    "Table II as the comparison baseline. From the experimental\n",
    "results, three observations can be found as follows.\n",
    "The first observation is that all the methods achieved much\n",
    "more profits in the trending market. Since we have allowed\n",
    "short operation in trading, the trader can also make money in\n",
    "the downward market. This can be denmonstrated from the\n",
    "IF data that Chinese market exhibits an increasing trend in\n",
    "the first, followed by a sudden drop. FDDR makes profits in\n",
    "either case. It is also observed that the P&L curve suffers\n",
    "a drawback during the transition period from the increasing\n",
    "trend to the decreasing trend. This is possible due to\n",
    "the significant differences in the training and testing data.\n",
    "In general, the RL model is particularly suitable to be applied\n",
    "to the trending market condition.\n",
    "Second, FDDR and DDR generally outperform the other\n",
    "two competitors on all the markets. SCOT also gains better\n",
    "performance than DRL in most conditions. This observation\n",
    "verifies that feature learning indeed contributes in\n",
    "improving the trading performance. Besides, the DL methods\n",
    "\n",
    "(FDDR and DDR) make more profits with higher SR on\n",
    "all the tests than the shallow learning approach (SCOT).\n",
    "Among the two DL methods, adding an extra layer for fuzzy\n",
    "representation seems to be a good way to further improve the\n",
    "results. This claim can be easily verified from Table II and\n",
    "Fig. 6 in which FDDR wins the DDR on all the tests except\n",
    "the one in Fig. 6(g). As claimed in the last paragraph, the\n",
    "DRL is a trend following system that may suffer losses on\n",
    "the market with small volatility. However, it is observed from\n",
    "the results that even on the nontrending period, e.g., on the\n",
    "SU data or in the early period of the IF data, FDDR is also\n",
    "effective to make the positive accumulation from the swing\n",
    "market patterns. Such finding successfully verifies another\n",
    "important property of fuzzy learning in reducing market\n",
    "uncertainties.\n",
    "Third, exploiting SR as the RL objective always leads to\n",
    "more reliable performances. Such reliability can be observed\n",
    "from both the SR quantity in Table II and the shapes of the\n",
    "P&L curves in Fig. 6. It is observed from Table II that the\n",
    "highest profits on the IF data were made by optimizing the TP\n",
    "as the objective in DDR. However, the SR on that testing\n",
    "condition is worse than the other. In portfolio management,\n",
    "rather than struggling for the highest profits with high risk, it\n",
    "is more intellectual to make good profits within acceptable risk\n",
    "level. Therefore, in practical usage, it is still recommended to\n",
    "use the SR as the reward function for RL.\n",
    "In conclusion, the RL framework is perhaps a trend-based\n",
    "trading strategy and could make reliable profits on the markets\n",
    "with large price movement (no matter in which direction).\n",
    "The DL-based trading systems generally outperform other\n",
    "DRL models either with or without shallow feature learning.\n",
    "By incorporating the fuzzy learning concept into the system,\n",
    "the FDDR can even generate good results in the nontrending\n",
    "period. When training a deep trading model, it is suggested to\n",
    "use the SR as the RL objective which balances the profit and\n",
    "the risk well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VI. CONCLUSION\n",
    "This paper introduces the contemporary DL into a typical\n",
    "DRL framework for financial signal processing and online\n",
    "trading. The contributions of the system are twofold. First, it is\n",
    "a technical-indicator-free trading system that greatly releases\n",
    "humans to select the features from a large amount of candidates.\n",
    "This advantage is due to the automatic feature learning\n",
    "mechanism of DL. In addition, by considering the nature of\n",
    "the financial signal, we have extended the fuzzy learning into\n",
    "the DL model to reduce the uncertainty in the original time\n",
    "series. The results on both the stock-index and commodity\n",
    "future contracts demonstrate the effectiveness of the learning\n",
    "system in simultaneous market condition summarization and\n",
    "This article has been accepted for inclusion in a future issue of this journal. Content is final as presented, with the exception of pagination.\n",
    "DENG et al.: DDR LEARNING FOR FINANCIAL SIGNAL REPRESENTATION AND TRADING 11\n",
    "optimal action learning. To the best of our knowledge, this\n",
    "is the first attempt to use the DL with the real-time financial\n",
    "trading.\n",
    "While the power of the DDR system has been verified in\n",
    "this paper, there are some promising future directions. First,\n",
    "all the methods proposed in this paper only handle one share\n",
    "of the asset. In some large hedge funds, the trading systems\n",
    "are always required to be capable in managing a number\n",
    "of assets simultaneously. In the future, the DL framework\n",
    "will be extended to extract features from multiple asserts\n",
    "and to learn the portfolio management strategies. Second, the\n",
    "financial market is not stationary that may change in real time.\n",
    "The knowledge learned from the past training data may not\n",
    "sufficiently reflect the information of the subsequent testing\n",
    "period. The method to intelligently select the right training\n",
    "period is still an open problem in the field.\n",
    "REFERENCES\n",
    "[1] E. W. Saad, D. V. Prokhorov, and D. C. Wunsch, II, “Comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NIGG/anaconda/lib/python3.5/site-packages/pandas/io/data.py:33: FutureWarning: \n",
      "The pandas.io.data module is moved to a separate package (pandas-datareader) and will be removed from pandas in a future version.\n",
      "After installing the pandas-datareader package (https://github.com/pydata/pandas-datareader), you can change the import ``from pandas.io import data, wb`` to ``from pandas_datareader import data, wb``.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#　統計用ツール\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as tsa\n",
    "from patsy import dmatrices\n",
    "\n",
    "#描画\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "\n",
    "#株価\n",
    "import pandas as pd\n",
    "import pandas.io.data as web\n",
    "\n",
    "#深層学習\n",
    "import chainer\n",
    "from chainer import cuda, Function, gradient_check, Variable, optimizers, serializers, utils\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-10-02</th>\n",
       "      <td>15735.709961</td>\n",
       "      <td>15902.509766</td>\n",
       "      <td>15514.040039</td>\n",
       "      <td>15902.509766</td>\n",
       "      <td>0</td>\n",
       "      <td>15902.509766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-10-03</th>\n",
       "      <td>15905.250000</td>\n",
       "      <td>15956.450195</td>\n",
       "      <td>15779.540039</td>\n",
       "      <td>15912.089844</td>\n",
       "      <td>0</td>\n",
       "      <td>15912.089844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-10-04</th>\n",
       "      <td>15906.179688</td>\n",
       "      <td>16153.669922</td>\n",
       "      <td>15808.660156</td>\n",
       "      <td>16149.080078</td>\n",
       "      <td>0</td>\n",
       "      <td>16149.080078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-10-05</th>\n",
       "      <td>16157.200195</td>\n",
       "      <td>16192.780273</td>\n",
       "      <td>16052.120117</td>\n",
       "      <td>16099.259766</td>\n",
       "      <td>0</td>\n",
       "      <td>16099.259766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-10-06</th>\n",
       "      <td>16084.049805</td>\n",
       "      <td>16084.049805</td>\n",
       "      <td>15884.650391</td>\n",
       "      <td>15994.240234</td>\n",
       "      <td>0</td>\n",
       "      <td>15994.240234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Open          High           Low         Close  Volume  \\\n",
       "Date                                                                         \n",
       "2000-10-02  15735.709961  15902.509766  15514.040039  15902.509766       0   \n",
       "2000-10-03  15905.250000  15956.450195  15779.540039  15912.089844       0   \n",
       "2000-10-04  15906.179688  16153.669922  15808.660156  16149.080078       0   \n",
       "2000-10-05  16157.200195  16192.780273  16052.120117  16099.259766       0   \n",
       "2000-10-06  16084.049805  16084.049805  15884.650391  15994.240234       0   \n",
       "\n",
       "               Adj Close  \n",
       "Date                      \n",
       "2000-10-02  15902.509766  \n",
       "2000-10-03  15912.089844  \n",
       "2000-10-04  16149.080078  \n",
       "2000-10-05  16099.259766  \n",
       "2000-10-06  15994.240234  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = '2000-10-01'\n",
    "end = '2016-10-01'\n",
    "\n",
    "p = web.DataReader('^N225', 'yahoo', start, end)\n",
    "p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "price = p[\"Adj Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "price = price.pct_change()[1:] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "price = list(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = np.matrix([price[i:i+50] for i in range(len(price) - 50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rho = 1\n",
    "c_0 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tanh(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "R = delta*z - c*np.abs(delta - delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(1-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kms = KMeans(n_clusters=3).fit_predict(f[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.01428269,  0.98228509,  0.99655113,  0.99608644,  0.99046394,\n",
       "        1.02268784,  0.98348144,  1.02641543,  0.97730243,  0.9803858 ,\n",
       "        1.02719169,  1.03042707,  0.96803347,  1.01002362,  0.9764374 ,\n",
       "        1.02197054,  0.98022297,  1.01072269,  1.01336307,  1.01760932,\n",
       "        0.97534862,  1.03838272,  0.96332872,  1.0059021 ,  0.97418176,\n",
       "        1.01769367,  0.98305799,  1.02176659,  1.00980508,  0.976403  ,\n",
       "        1.01156901,  1.00206568,  0.99238571,  1.00104966,  1.00848143,\n",
       "        1.02728559,  0.96842021,  0.9938368 ,  1.02023548,  1.0030142 ,\n",
       "        0.9953541 ,  0.97479007,  1.0311285 ,  0.97574616,  1.00984251,\n",
       "        1.02337691,  0.98519189,  0.997006  ,  0.98057386,  0.99065623])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(f[0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0013786862889182"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(f[0])[0][kms == 0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NIGG/anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "n_samples=1 should be >= n_clusters=3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-83cf78db1e1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/NIGG/anaconda/lib/python3.5/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \"\"\"\n\u001b[1;32m    880\u001b[0m         \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minertia_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/NIGG/anaconda/lib/python3.5/site-packages/sklearn/cluster/k_means_.py\u001b[0m in \u001b[0;36m_check_fit_data\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             raise ValueError(\"n_samples=%d should be >= n_clusters=%d\" % (\n\u001b[0;32m--> 859\u001b[0;31m                 X.shape[0], self.n_clusters))\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: n_samples=1 should be >= n_clusters=3"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(f[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 0, 0, 0, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2,\n",
       "       2, 2, 2, 0, 0, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 0, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 0, 2, 0, 2, 2, 0, 2, 2, 2, 2,\n",
       "       2, 0, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 0,\n",
       "       2, 0, 0, 0, 0, 0, 2, 2, 0, 2, 2, 0, 0, 0, 2, 0, 0, 2, 2, 2, 2, 2, 0,\n",
       "       2, 0, 2, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 2, 2, 2,\n",
       "       2, 0, 2, 2, 0, 0, 2, 2, 0, 2, 0, 2, 0, 0, 0, 2, 2, 0, 0, 2, 0, 0, 0,\n",
       "       0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 2, 2, 0, 0, 0, 2, 0, 2,\n",
       "       0, 2, 2, 2, 2, 2, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 2, 2, 0, 0, 2, 0, 0,\n",
       "       1, 2, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 0, 2,\n",
       "       0, 2, 0, 2, 0, 0, 2, 0, 0, 0, 0, 2, 1, 2, 0, 2, 0, 2, 0, 2, 2, 2, 2,\n",
       "       0, 0, 0, 0, 0, 2, 2, 0, 2, 0, 2, 0, 0, 2, 2, 0, 2, 0, 0, 2, 2, 0, 0,\n",
       "       2, 0, 0, 2, 0, 2, 2, 0, 0, 0, 0, 2, 1, 2, 2, 2, 2, 2, 0, 0, 2, 0, 0,\n",
       "       0, 0, 0, 2, 0, 0, 0, 2, 0, 0, 0, 2, 2, 1, 2, 2, 0, 0, 0, 2, 0, 2, 2,\n",
       "       0, 0, 0, 0, 0, 1, 2, 2, 1, 0, 2, 1, 0, 2, 0, 0, 2, 0, 1, 0, 0, 1, 0,\n",
       "       2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 1, 1, 2, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 2, 1, 1, 0, 0, 2, 1, 1, 0, 2,\n",
       "       0, 0, 2, 2, 1, 0, 1, 0, 2, 0, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 2, 1, 2, 0, 0, 1, 2, 2, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 2, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 2, 1,\n",
       "       1, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 2, 1, 0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 0, 2, 1, 1, 0, 1, 0, 1, 2,\n",
       "       0, 0, 1, 1, 1, 0, 1, 2, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 2, 1, 1,\n",
       "       0, 0, 1, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 2, 1, 1,\n",
       "       1, 2, 2, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 2, 1, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 2,\n",
       "       0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 2, 1, 0, 1, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 0, 2, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 2, 1, 2,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    ">>> kmeans.labels_\n",
    "array([0, 0, 0, 1, 1, 1], dtype=int32)\n",
    ">>> kmeans.predict([[0, 0], [4, 4]])\n",
    "array([0, 1], dtype=int32)\n",
    ">>> kmeans.cluster_centers_\n",
    "array([[ 1.,  2.],\n",
    "       [ 4.,  2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def o():\n",
    "    return 1/(1+np.exp(-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fuzzy():\n",
    "    return np.exp(-1*(a - m)**2/sigma**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FRDNN(object):\n",
    "    def __init__(self, data, rho, c_0):\n",
    "        self.data = data\n",
    "        self.rho = rho\n",
    "        self.c_0 = c_0\n",
    "    \n",
    "    def _fuzzy_(self):\n",
    "        self.vf = ji\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7200759760208889e-44"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-1*(100 - 99)**2/0.1**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CAR(Chain):\n",
    "    def __init__(self, unit1, unit2, unit3, col_num):\n",
    "        self.unit1 = unit1\n",
    "        self.unit2 = unit2\n",
    "        self.unit3 = unit3\n",
    "        super(CAR, self).__init__(\n",
    "            l1 = L.Linear(col_num, unit1),\n",
    "            l2 = L.Linear(self.unit1, self.unit1),\n",
    "            l3 = L.Linear(self.unit1, self.unit2),\n",
    "            l4 = L.Linear(self.unit2, self.unit3),\n",
    "            l5 = L.Linear(self.unit3, self.unit3),\n",
    "            l6 = L.Linear(self.unit3, 1),\n",
    "        )\n",
    "    \n",
    "    def __call__(self, x, y):\n",
    "        fv = self.fwd(x, y)\n",
    "        loss = F.mean_squared_error(fv, y)\n",
    "        return loss\n",
    "    \n",
    "    def fwd(self, x, y):\n",
    "        h1 = F.sigmoid(self.l1(x))\n",
    "        h2 = F.sigmoid(self.l2(h1))\n",
    "        h3 = F.sigmoid(self.l3(h2))\n",
    "        h4 = F.sigmoid(self.l4(h3))\n",
    "        h5 = F.sigmoid(self.l5(h4))\n",
    "        h6 = self.l6(h5)\n",
    "        return h6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def DL(df, n, bs = 200):\n",
    "    dum1 = pd.DataFrame((df['pay'] < 100000)*1)\n",
    "    dum1.columns = ['low']\n",
    "    dum2 = pd.DataFrame((df['pay'] > 150000)*1)\n",
    "    dum2.columns = ['high']\n",
    "    dum = pd.concat((dum1, dum2), axis=1)\n",
    "    \n",
    "    df_with_dummy = pd.concat((df, dum), axis=1)\n",
    "    \n",
    "    cluster_array = np.array([df['square'], df['fX']*1000, df['fY']*1000])\n",
    "    gmm = mixture.GaussianMixture(n_components=n, covariance_type='full').fit(cluster_array.T)\n",
    "    dum = pd.get_dummies(gmm.predict(cluster_array.T))\n",
    "    dum_nam = ['d%s'%i for i in range(n)] \n",
    "    dum.columns = dum_nam\n",
    "    \n",
    "    df_with_dummy = pd.concat((df_with_dummy, dum), axis=1)\n",
    "\n",
    "    vars = ['pay', 'square', 'k', 'lk', 'dk', 'sdk', 'sldk', 'south_direction_dummy', 'building_year', \n",
    "            'new_dummy', 'mansyon_dumy', 'teiki_syakuya_dummy', 'walk_minute_dummy', 'r', 'rc_dummy', \n",
    "            'room_nums', 'low', 'high']\n",
    "    vars = vars + dum_nam[:-1]\n",
    "\n",
    "    eq = fml_build(vars)\n",
    "\n",
    "    y, X = dmatrices(eq, data=df_with_dummy, return_type='dataframe')\n",
    "    \n",
    "    y_in = y[1:1000]\n",
    "    X_in = X[1:1000]\n",
    "    \n",
    "    y_ex = y[1000:]\n",
    "    X_ex = X[1000:]\n",
    "\n",
    "    logy_in = np.log(y_in)\n",
    "    \n",
    "    logy_in = np.array(logy_in, dtype='float32')\n",
    "    X_in = np.array(X_in, dtype='float32')\n",
    "    \n",
    "    y = Variable(logy_in)\n",
    "    x = Variable(X_in)\n",
    "    \n",
    "    num, col_num = X_in.shape\n",
    "\n",
    "    model1 = CAR(10, 10, 3, col_num)\n",
    "    optimizer = optimizers.SGD()\n",
    "    optimizer.setup(model1)\n",
    "    \n",
    "    for j in range(1000):\n",
    "        sffindx = np.random.permutation(num)\n",
    "        for i in range(0, num, bs):\n",
    "            x = Variable(X_in[sffindx[i:(i+bs) if (i+bs) < num else num]])\n",
    "            y = Variable(logy_in[sffindx[i:(i+bs) if (i+bs) < num else num]])\n",
    "            model1.zerograds()\n",
    "            loss = model1(x, y)\n",
    "            loss.backward()\n",
    "            optimizer.update()\n",
    "        if j % 1000 == 0:\n",
    "            loss_val = loss.data\n",
    "            print('epoch:', j)\n",
    "            print('train mean loss={}'.format(loss_val))\n",
    "            print(' - - - - - - - - - ')\n",
    "    \n",
    "    return model1, np.array(y_ex, dtype='float32').reshape(len(y_ex)), np.array(X_ex, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results, y_ex, X_ex = DL(df, 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
